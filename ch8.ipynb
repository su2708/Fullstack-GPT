{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 HuggingFaceEndpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACEHUB_API_KEY = os.getenv(\"HUGGINGFACEHUB_API_KEY\")\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"  # 라이브러리끼리의 충돌을 해결\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\USER\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "\n",
      "\n",
      "A potato is a starchy, tuberous crop from the Solanum tuberosum species. The plant is native to the Andean region of South America, and was the first vegetable to be cultivated for food in the New World.\n",
      "\n",
      "Potatoes are grown worldwide and are a major source of food for many people. They come in a variety of shapes, sizes, and colors, and can be cooked in many different ways, such as boiling, baking, frying, and mashing.\n",
      "\n",
      "Potatoes are a good source of carbohydrates, fiber, and various vitamins and minerals, including vitamin C, potassium, and vitamin B6. They are also low in fat and calories, making them a popular choice for people on weight-loss diets.\n",
      "\n",
      "In addition to being a food source, potatoes have also been used for a variety of other purposes throughout history. For example, they have been used as a currency, a medicine, and a dye. Today, potatoes are an important crop in agriculture and are used in a wide range of products, such as potato chips, french fries, and potato flour.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# HuggingFace 모델 선택\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",  # 선택한 모델의 ID\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_KEY,\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is the meaning of {word}?\")\n",
    "\n",
    "chain = prompt | llm\n",
    "result = chain.invoke({\"word\": \"potato\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "cuda\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.__version__)\n",
    "print(device)\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3_envs\\Fullstack-GPT\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A banana is a special type of fruit, typically about 20 percent larger than an ordinary banana and about 4 centimeters (5 meters) long. Because of this, bananas are usually cooked in large pots for a day or two or occasionally taken into the hospital after a surgery.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# HuggingFace 모델을 다운로드\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"openai-community/gpt2\",  # 사용할 모델의 ID\n",
    "    task=\"text-generation\",  # 수행할 작업 지정\n",
    "    device=0,  # 0: GPU, -1: CPU (default)\n",
    "    \n",
    "    # 파이프라인에 전달할 추가 인자 설정 \n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\":50\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "\n",
    "chain = prompt | llm\n",
    "result = chain.invoke({\"word\": \"banana\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 4.66G/4.66G [01:58<00:00, 39.5MiB/s]\n",
      "Verifying: 100%|██████████| 4.66G/4.66G [00:58<00:00, 79.7MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are powerful AI models that require significant computational resources to train and run. However, with some optimization techniques and hardware considerations, you can still run LLMs efficiently on your laptop:\n",
      "\n",
      "1. **Choose the right model**: Select a smaller or more efficient language model variant, such as BERT-base instead of BERT-large.\n",
      "2. **Use a GPU (if available)**: If your laptop has a dedicated graphics card, consider using it to accelerate computations. Many LLMs are designed to run on GPUs, which can significantly speed up processing times.\n",
      "3. **Optimize model inputs**: Reduce the input sequence length or use attention-based models that process shorter sequences more efficiently.\n",
      "4. **Use batch processing**: Process multiple examples simultaneously (batch size) instead of running single instances one by one. This can reduce overall computation time and memory usage.\n",
      "5. **Take advantage of parallelization**: Utilize multi-core processors to run computations in parallel, which can speed up processing times.\n",
      "6. **Limit model depth**: Reduce the number of layers or use shallower models that require fewer computational resources.\n",
      "7. **Use a cloud-based service (if needed)**: If your laptop's hardware is not sufficient for running LLMs efficiently, consider using cloud services like Google Colab, AWS SageMaker, or Azure Machine Learning to access more powerful computing resources.\n",
      "\n",
      "To further optimize performance on your laptop:\n",
      "\n",
      "1. **Close unnecessary applications**: Free up system resources by closing other resource-intensive programs.\n",
      "2. **Adjust power settings**: Consider reducing the screen brightness and turning off unnecessary features (e.g., Bluetooth) to conserve energy and reduce heat generation.\n",
      "3. **Update drivers and software**: Ensure that your GPU driver, operating system, and other relevant software are updated for optimal performance.\n",
      "\n",
      "Some popular LLMs with smaller footprints include:\n",
      "\n",
      "* DistilBERT: A compact version of BERT\n",
      "* RoBERTa-base: A more efficient variant of the original RoBERTa model\n",
      "* DebertaV2-small: A small-sized language model based on DeBERT\n",
      "\n",
      "Remember to always check the specific requirements and recommendations for each LLM you want to run, as they may have unique optimization strategies.\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "model = GPT4All(\n",
    "    model_name=\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\",\n",
    "    model_path=\"./files/gpt4all\",\n",
    ") # downloads / loads a 4.66GB LLM\n",
    "\n",
    "with model.chat_session():\n",
    "    print(model.generate(\"How can I run LLMs efficiently on my laptop?\", max_tokens=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
